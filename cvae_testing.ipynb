{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4bc3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import ConditionalVAE\n",
    "from model.dataset import TabularDataset\n",
    "from model.utils.visualization import PCA_plot, PCA_plot_rare_on_top\n",
    "from model.utils.optimization import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from matplotlib.cm import get_cmap\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from imblearn.over_sampling import (\n",
    "\tSMOTE,\n",
    "\tADASYN,\n",
    "\tBorderlineSMOTE,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def mask_features(x, min_mask: int = 1, max_mask: int = 3):\n",
    "\tx_masked = x.clone()\n",
    "\tfor i in range(x.size(0)):\n",
    "\t\tk = torch.randint(min_mask, max_mask + 1, (1,)).item()\n",
    "\t\tidx = torch.randperm(x.size(1))[:k]\n",
    "\t\tx_masked[i, idx] = 0\n",
    "\treturn x_masked\n",
    "\n",
    "def execute(x_train, y_train, x_validation, y_validation, do_print=False):\n",
    "\tx_prior = np.concatenate((x_train, x_validation), axis=0)\n",
    "\ty_prior = np.concatenate((y_train, y_validation), axis=0)\n",
    "\n",
    "\tx_smote, y_smote = SMOTE().fit_resample(x_prior, y_prior)\n",
    "\n",
    "\tminority_label = pd.DataFrame(y_train).value_counts().argmin()\n",
    "\tminority_indices = np.where(y_train==minority_label)[0]\n",
    "\tminority_features = x_train[minority_indices]\n",
    "\tminority_labels = y_train[minority_indices]\n",
    "\n",
    "\tnum_features = x_prior[0].shape[0]\n",
    "\n",
    "\tnearest_neighbours = NearestNeighbors(n_neighbors=5, metric=\"euclidean\").fit(x_smote)\n",
    "\tdist, idx = nearest_neighbours.kneighbors(x_smote)\n",
    "\n",
    "\tdist = dist[:, 1:]          # shape: (n_samples, k)\n",
    "\tidx  = idx[:, 1:]           # shape: (n_samples, k)\n",
    "\n",
    "\tknn_features = [x_smote[row_idx] for row_idx in idx]\n",
    "\tknn_labels = [y_smote[row_idx] for row_idx in idx]\n",
    "\n",
    "\tinput_set = []\n",
    "\trecon_set = []\n",
    "\tlabels = []\n",
    "\tfor s_idx, sample in enumerate(x_smote):\n",
    "\t\t\n",
    "\t\tfor n_idx, neighbouring_sample in enumerate(knn_features[s_idx]):\n",
    "\t\t\tif y_smote[s_idx] == knn_labels[s_idx][n_idx]:\n",
    "\t\t\t\tinput_set.append(sample)\n",
    "\t\t\t\trecon_set.append(neighbouring_sample)\n",
    "\t\t\t\tlabels.append(y_smote[s_idx])\n",
    "\n",
    "\th1 = num_features + (num_features//2)\n",
    "\th2 = num_features * 2\n",
    "\tlatent_dim = 20\n",
    "\n",
    "\tcvae = ConditionalVAE(\n",
    "\t\tinput_dim=num_features, \n",
    "\t\th1=h1,\n",
    "\t\th2 = h2,\n",
    "\t\tlatent_dim=latent_dim).to(device)\n",
    "\n",
    "\tepochs = 900\n",
    "\tbatch_size = 32\n",
    "\tlr = 1e-3\n",
    "\tbeta = 0.8\n",
    "\n",
    "\tdata = TabularDataset(x_smote, x_smote, y_smote)\n",
    "\tloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\tcvae.train()\n",
    "\topt = optim.Adam(cvae.parameters(), lr=lr)\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\trunning = 0\n",
    "\t\tfor encode_in, decode_comp, label in loader:\n",
    "\t\t\txb = encode_in.float().to(device)\n",
    "\t\t\tyb = decode_comp.float().to(device)\n",
    "\t\t\tlabel = label.float().to(device)\n",
    "\n",
    "\t\t\t# xb_mask = mask_features(xb)\n",
    "\t\t\t\n",
    "\t\t\trecon, mu, logvar = cvae(xb, label)\n",
    "\t\t\t# recon_loss = nn.MSELoss()(recon, xb)\n",
    "\t\t\t# kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\t# loss = recon_loss + (kl_div*beta)\n",
    "\t\t\trecon_loss = nn.MSELoss(reduction='sum')(recon, xb)\n",
    "\t\t\tkl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\tloss = (recon_loss + beta * kl_div) / xb.size(0)   # per-batch average\n",
    "\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt.step()\n",
    "\n",
    "\t\t\trunning += loss.item()\n",
    "\t\t\n",
    "\t\tif (epoch % 5 == 0 or epoch == epochs) and do_print:\n",
    "\t\t\tprint(f\"Epoch {epoch:03d} | loss: {running / len(loader):.4f}\")\n",
    "\n",
    "\tepochs = 400\n",
    "\n",
    "\tdata = TabularDataset(\n",
    "\t\tnp.array(input_set), \n",
    "\t\tnp.array(recon_set), \n",
    "\t\tnp.array(labels)\n",
    "\t)\n",
    "\tloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\tcvae.train()\n",
    "\topt = optim.Adam(cvae.parameters(), lr=lr)\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\trunning = 0\n",
    "\t\tfor encode_in, decode_comp, label in loader:\n",
    "\t\t\txb = encode_in.float().to(device)\n",
    "\t\t\txb_masked = mask_features(xb)\n",
    "\t\t\tyb = decode_comp.float().to(device)\n",
    "\t\t\tlabel = label.float().to(device)\n",
    "\t\t\t# xb_mask = mask_features(xb)\n",
    "\n",
    "\t\t\trecon, mu, logvar = cvae(xb_masked, label)\n",
    "\t\t\t# recon_loss = nn.MSELoss()(recon, xb)\n",
    "\t\t\t# kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\t# loss = recon_loss + (kl_div*beta)\n",
    "\t\t\trecon_loss = nn.MSELoss(reduction='sum')(recon, yb)\n",
    "\t\t\tkl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\tloss = (recon_loss + beta * kl_div) / xb.size(0)   # per-batch average\n",
    "\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt.step()\n",
    "\n",
    "\t\t\trunning += loss.item()\n",
    "\n",
    "\t\tif (epoch % 5 == 0 or epoch == epochs) and do_print:\n",
    "\t\t\tprint(f\"Epoch {epoch:03d} | loss: {running / len(loader):.4f}\")\n",
    "\n",
    "\tfeature_variance = np.var(x_prior, axis=0)\n",
    "\tfeature_mins = np.min(x_prior, axis=0)\n",
    "\tfeature_maxs = np.max(x_prior, axis=0)\n",
    "\tfeature_grids = []\n",
    "\tfor idx, var in enumerate(feature_variance):\n",
    "\t\tlo  = feature_mins[idx] - var      # lower bound  (min – variance)\n",
    "\t\thi  = feature_maxs[idx] + var      # upper bound  (max + variance)\n",
    "\t\tstep = var * 2 or 1e-8             # avoid step == 0 if var == 0\n",
    "\t\tgrid = np.arange(lo, hi + step, step)\n",
    "\t\tfeature_grids.append(grid)\n",
    "\n",
    "\t# --- Cartesian product  -------------------------------------------------\n",
    "\t# itertools.product is lazy ⇒ less memory than meshgrid on huge spaces\n",
    "\tsynthetic_X = np.fromiter(\n",
    "\t\t(val for combo in product(*feature_grids) for val in combo),\n",
    "\t\tdtype=float\n",
    "\t).reshape(-1, len(feature_grids))\n",
    "\n",
    "\tif do_print:\n",
    "\t\tprint(f\"{synthetic_X.shape[0]:,} synthetic rows × {synthetic_X.shape[1]} features\")\n",
    "\n",
    "\ttest_labels = torch.tensor([minority_label] * synthetic_X.shape[0]).to(device)\n",
    "\n",
    "\tcvae.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\txb = torch.tensor(synthetic_X).float().to(device)\n",
    "\t\tmu, logvar = cvae.encode(xb)\n",
    "\t\tz = cvae.reparameterize(mu, logvar)\n",
    "\t\tsynthetic_X_DECODE = cvae.decode(z, test_labels).to('cpu')\n",
    "\n",
    "\tsynthetic_Y = torch.tensor([minority_label] * synthetic_X.shape[0]).numpy()\n",
    "\n",
    "\tproblem = NSGA_II_Filter(\n",
    "\t\tnp.concatenate((x_prior, synthetic_X_DECODE), axis=0), \n",
    "\t\tnp.concatenate((y_prior, synthetic_Y), axis=0), \n",
    "\t\tx_validation, y_validation,\n",
    "\t)\n",
    "\talgorithm = NSGA2(\n",
    "\t\tpop_size=500, \n",
    "\t\tsampling=DiverseSampling(), \n",
    "\t\tcrossover=HUX(), \n",
    "\t\tmutation=BitflipMutation(), \n",
    "\t\teliminate_duplicates=True\n",
    "\t)\n",
    "\tresult = minimize(\n",
    "\t\tproblem, \n",
    "\t\talgorithm, \n",
    "\t\t('n_gen', 10),\n",
    "\t\tsave_history=False,\n",
    "\t)\n",
    "\n",
    "\tcandidate_x = np.concatenate((x_prior, synthetic_X_DECODE), axis=0)\n",
    "\tcandidate_y = np.concatenate((y_prior, synthetic_Y), axis=0)\n",
    "\n",
    "\tmax_validation_auc = -1\n",
    "\tbest_x = None\n",
    "\tbest_y = None\n",
    "\n",
    "\tfor x in result.X:\n",
    "\t\tfiltered_x = candidate_x[x]\n",
    "\t\tfiltered_y = candidate_y[x]\n",
    "\t\t\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=5)\n",
    "\t\tmodel.fit(filtered_x, filtered_y)\n",
    "\t\ty_pred = model.predict(x_validation)\n",
    "\t\tauc = roc_auc_score(y_validation, y_pred)\n",
    "\n",
    "\t\tif auc > max_validation_auc:\n",
    "\t\t\tbest_x = filtered_x\n",
    "\t\t\tbest_y = filtered_y\n",
    "\t\t\tmax_validation_auc = auc\n",
    "\n",
    "\treturn best_x, best_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f228e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as fh:\n",
    "\tdata_mapper = pickle.load(fh)\n",
    "splits = pd.read_csv('data_splits.csv')\n",
    "\n",
    "data_keys = []\n",
    "for split_name in splits:\n",
    "\tfor idx in range(31):\n",
    "\t\tdata_keys.append(f\"{idx}_{split_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a2101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_glass-0-6_vs_5\n",
      "1_glass-0-6_vs_5\n",
      "2_glass-0-6_vs_5\n",
      "3_glass-0-6_vs_5\n",
      "4_glass-0-6_vs_5\n",
      "5_glass-0-6_vs_5\n",
      "6_glass-0-6_vs_5\n",
      "7_glass-0-6_vs_5\n",
      "8_glass-0-6_vs_5\n",
      "9_glass-0-6_vs_5\n",
      "10_glass-0-6_vs_5\n",
      "11_glass-0-6_vs_5\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in splits:\n",
    "\tif dataset_name in ['cleveland-0_vs_4']: continue\n",
    "\tfor idx in range(31):\n",
    "\t\tdata_key = f\"{idx}_{dataset_name}\"\n",
    "\t\tif os.path.exists(f'results_new/{data_key}.pkl'): continue\n",
    "\t\t\n",
    "\t\t# for _ in range(3):\n",
    "\t\t# \ttry:\n",
    "\t\tprint(data_key)\n",
    "\t\tresample_x, resample_y = execute(\n",
    "\t\t\tdata_mapper[data_key]['x_train'],\n",
    "\t\t\tdata_mapper[data_key]['y_train'],\n",
    "\t\t\tdata_mapper[data_key]['x_validation'],\n",
    "\t\t\tdata_mapper[data_key]['y_validation']\n",
    "\t\t)\n",
    "\t\twith open(f'results_new/{data_key}.pkl', 'wb') as fh:\n",
    "\t\t\tpickle.dump((resample_x, resample_y), fh)\n",
    "\t\t\t# \tbreak\n",
    "\t\t\t# except Exception as e:\n",
    "\t\t\t# \tprint(e)\n",
    "\t\t\t# \tcontinue\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "156377c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 503810 / 504000 synthetic points.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- your grid construction (unchanged) --------------------------------\n",
    "feature_variance = np.var(x_prior, axis=0)\n",
    "feature_mins     = np.min(x_prior, axis=0)\n",
    "feature_maxs     = np.max(x_prior, axis=0)\n",
    "\n",
    "feature_grids = []\n",
    "for idx, var in enumerate(feature_variance):\n",
    "    lo   = feature_mins[idx] - var\n",
    "    hi   = feature_maxs[idx] + var\n",
    "    step = var * 2 or 1e-8       # avoid 0 step if var==0\n",
    "    grid = np.arange(lo, hi + step, step)\n",
    "    feature_grids.append(grid)\n",
    "\n",
    "# Cartesian product → all combinations\n",
    "synthetic_X = np.fromiter(\n",
    "    (val for combo in product(*feature_grids) for val in combo),\n",
    "    dtype=float\n",
    ").reshape(-1, len(feature_grids))\n",
    "\n",
    "# --- prune points too close to existing samples ------------------------\n",
    "# Radius in *standard deviations* after z-scoring (e.g., 0.75 ~ within 0.75 SD)\n",
    "radius_std = 2\n",
    "\n",
    "# 1) scale to unit variance so distance is comparable across features\n",
    "scaler = StandardScaler().fit(x_prior)\n",
    "Xz = scaler.transform(x_prior)\n",
    "Sz = scaler.transform(synthetic_X)\n",
    "\n",
    "# 2) remove synthetic points that have any neighbor within the radius\n",
    "nn = NearestNeighbors(algorithm=\"auto\").fit(Xz)\n",
    "# radius_neighbors returns, for each query row, indices of neighbors within 'radius'\n",
    "neighbors_within = nn.radius_neighbors(Sz, radius=radius_std, return_distance=False)\n",
    "\n",
    "keep_mask = np.fromiter((len(ix) == 0 for ix in neighbors_within),\n",
    "                        dtype=bool, count=Sz.shape[0])\n",
    "synthetic_X_pruned = synthetic_X[keep_mask]\n",
    "\n",
    "print(f\"Kept {synthetic_X_pruned.shape[0]} / {synthetic_X.shape[0]} synthetic points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4918a502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.5357142857142857),\n",
       " 0.9777397260273972,\n",
       " np.float64(0.5357142857142857))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_prior = np.concatenate((data_mapper[data_key]['x_train'], data_mapper[data_key]['x_validation']), axis=0)\n",
    "y_prior = np.concatenate((data_mapper[data_key]['y_train'], data_mapper[data_key]['y_validation']), axis=0)    \n",
    "x_smote, y_smote = SMOTE().fit_resample(x_prior, y_prior)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(x_prior, y_prior)\n",
    "y_pred = model.predict(x_test)\n",
    "roc_auc_score(y_test, y_pred), accuracy_score(y_test, y_pred), balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff7fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7497493734335839 0.9195205479452054 0.7497493734335839\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(x_smote, y_smote)\n",
    "y_pred = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(auc, acc, bal_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516777f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76203007518797 0.9434931506849316 0.7620300751879698\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(resample_x, resample_y)\n",
    "y_pred = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(auc, acc, bal_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a055037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t> B vs S pval 0.8327479062272571\n",
      "\t> B vs C pval 0.45132752491681716\n",
      "\t> S vs C pval 0.4222761086468382\n",
      "\t> S: 0.9083450210378682\n",
      "\t> B: 0.9018934081346425\n",
      "\t> C: 0.8930575035063114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"ecoli-0-4-6_vs_5\"\n",
    "\n",
    "baselines = []\n",
    "smoted = []\n",
    "for idx in range(31):\n",
    "\tdata_key = f\"{idx}_{dataset_name}\"\n",
    "\tx_train = data_mapper[data_key]['x_train']\n",
    "\ty_train = data_mapper[data_key]['y_train']\n",
    "\n",
    "\tx_validation = data_mapper[data_key]['x_validation']\n",
    "\ty_validation = data_mapper[data_key]['y_validation']\n",
    "\n",
    "\tx_test = data_mapper[data_key]['x_test']\n",
    "\ty_test = data_mapper[data_key]['y_test']\n",
    "\n",
    "\tx_prior = np.concatenate((x_train, x_validation), axis=0)\n",
    "\ty_prior = np.concatenate((y_train, y_validation), axis=0)  \n",
    "\n",
    "\tmodel = KNeighborsClassifier(n_neighbors=5)\n",
    "\tmodel.fit(x_prior, y_prior)\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tbaselines.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "\ttry:\n",
    "\t\tx_smote, y_smote = SMOTE().fit_resample(x_prior, y_prior)\n",
    "\n",
    "\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=5)\n",
    "\t\tmodel.fit(x_smote, y_smote)\n",
    "\t\ty_pred = model.predict(x_test)\n",
    "\t\tsmoted.append(roc_auc_score(y_test, y_pred))\n",
    "\texcept:\n",
    "\t\tsmoted.append(0)\n",
    "\n",
    "print(f\"\\t> B vs S pval {ranksums(baselines, smoted).pvalue}\")\n",
    "print(f\"\\t> B vs C pval {ranksums(baselines, cvae).pvalue}\")\n",
    "print(f\"\\t> S vs C pval {ranksums(smoted, cvae).pvalue}\")\n",
    "print(f\"\\t> S: {np.mean(smoted)}\")\n",
    "print(f\"\\t> B: {np.mean(baselines)}\")\n",
    "print(f\"\\t> C: {np.mean(cvae)}\")\n",
    "print(\"\")\n",
    "\n",
    "\t\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
