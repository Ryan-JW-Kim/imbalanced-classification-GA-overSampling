{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import ConditionalVAE\n",
    "from model.dataset import TabularDataset\n",
    "from model.utils.visualization import PCA_plot, PCA_plot_rare_on_top\n",
    "from model.utils.optimization import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from matplotlib.cm import get_cmap\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from imblearn.over_sampling import (\n",
    "\tSMOTE,\n",
    "\tADASYN,\n",
    "\tBorderlineSMOTE,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def mask_features(x, min_mask: int = 1, max_mask: int = 3):\n",
    "\tx_masked = x.clone()\n",
    "\tfor i in range(x.size(0)):\n",
    "\t\tk = torch.randint(min_mask, max_mask + 1, (1,)).item()\n",
    "\t\tidx = torch.randperm(x.size(1))[:k]\n",
    "\t\tx_masked[i, idx] = 0\n",
    "\treturn x_masked\n",
    "\n",
    "def execute(x_train, y_train, x_validation, y_validation, do_print=False):\n",
    "\tx_prior = np.concatenate((x_train, x_validation), axis=0)\n",
    "\ty_prior = np.concatenate((y_train, y_validation), axis=0)\n",
    "\n",
    "\tx_smote, y_smote = SMOTE().fit_resample(x_prior, y_prior)\n",
    "\n",
    "\tminority_label = pd.DataFrame(y_train).value_counts().argmin()\n",
    "\tminority_indices = np.where(y_train==minority_label)[0]\n",
    "\tminority_features = x_train[minority_indices]\n",
    "\tminority_labels = y_train[minority_indices]\n",
    "\n",
    "\tnum_features = x_prior[0].shape[0]\n",
    "\n",
    "\tnearest_neighbours = NearestNeighbors(n_neighbors=5, metric=\"euclidean\").fit(x_smote)\n",
    "\tdist, idx = nearest_neighbours.kneighbors(x_smote)\n",
    "\n",
    "\tdist = dist[:, 1:]          # shape: (n_samples, k)\n",
    "\tidx  = idx[:, 1:]           # shape: (n_samples, k)\n",
    "\n",
    "\tknn_features = [x_smote[row_idx] for row_idx in idx]\n",
    "\tknn_labels = [y_smote[row_idx] for row_idx in idx]\n",
    "\n",
    "\tinput_set = []\n",
    "\trecon_set = []\n",
    "\tlabels = []\n",
    "\tfor s_idx, sample in enumerate(x_smote):\n",
    "\t\t\n",
    "\t\tfor n_idx, neighbouring_sample in enumerate(knn_features[s_idx]):\n",
    "\t\t\tif y_smote[s_idx] == knn_labels[s_idx][n_idx]:\n",
    "\t\t\t\tinput_set.append(sample)\n",
    "\t\t\t\trecon_set.append(neighbouring_sample)\n",
    "\t\t\t\tlabels.append(y_smote[s_idx])\n",
    "\n",
    "\th1 = num_features + (num_features//2)\n",
    "\th2 = num_features * 2\n",
    "\tlatent_dim = 20\n",
    "\n",
    "\tcvae = ConditionalVAE(\n",
    "\t\tinput_dim=num_features, \n",
    "\t\th1=h1,\n",
    "\t\th2 = h2,\n",
    "\t\tlatent_dim=latent_dim).to(device)\n",
    "\n",
    "\tepochs = 900\n",
    "\tbatch_size = 32\n",
    "\tlr = 1e-3\n",
    "\tbeta = 0.8\n",
    "\n",
    "\tdata = TabularDataset(x_smote, x_smote, y_smote)\n",
    "\tloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\tcvae.train()\n",
    "\topt = optim.Adam(cvae.parameters(), lr=lr)\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\trunning = 0\n",
    "\t\tfor encode_in, decode_comp, label in loader:\n",
    "\t\t\txb = encode_in.float().to(device)\n",
    "\t\t\tyb = decode_comp.float().to(device)\n",
    "\t\t\tlabel = label.float().to(device)\n",
    "\n",
    "\t\t\t# xb_mask = mask_features(xb)\n",
    "\t\t\t\n",
    "\t\t\trecon, mu, logvar = cvae(xb, label)\n",
    "\t\t\t# recon_loss = nn.MSELoss()(recon, xb)\n",
    "\t\t\t# kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\t# loss = recon_loss + (kl_div*beta)\n",
    "\t\t\trecon_loss = nn.MSELoss(reduction='sum')(recon, xb)\n",
    "\t\t\tkl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\tloss = (recon_loss + beta * kl_div) / xb.size(0)   # per-batch average\n",
    "\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt.step()\n",
    "\n",
    "\t\t\trunning += loss.item()\n",
    "\t\t\n",
    "\t\tif (epoch % 5 == 0 or epoch == epochs) and do_print:\n",
    "\t\t\tprint(f\"Epoch {epoch:03d} | loss: {running / len(loader):.4f}\")\n",
    "\n",
    "\tepochs = 400\n",
    "\n",
    "\tdata = TabularDataset(\n",
    "\t\tnp.array(input_set), \n",
    "\t\tnp.array(recon_set), \n",
    "\t\tnp.array(labels)\n",
    "\t)\n",
    "\tloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\tcvae.train()\n",
    "\topt = optim.Adam(cvae.parameters(), lr=lr)\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\trunning = 0\n",
    "\t\tfor encode_in, decode_comp, label in loader:\n",
    "\t\t\txb = encode_in.float().to(device)\n",
    "\t\t\txb_masked = mask_features(xb)\n",
    "\t\t\tyb = decode_comp.float().to(device)\n",
    "\t\t\tlabel = label.float().to(device)\n",
    "\t\t\t# xb_mask = mask_features(xb)\n",
    "\n",
    "\t\t\trecon, mu, logvar = cvae(xb_masked, label)\n",
    "\t\t\t# recon_loss = nn.MSELoss()(recon, xb)\n",
    "\t\t\t# kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\t# loss = recon_loss + (kl_div*beta)\n",
    "\t\t\trecon_loss = nn.MSELoss(reduction='sum')(recon, yb)\n",
    "\t\t\tkl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\t\t\tloss = (recon_loss + beta * kl_div) / xb.size(0)   # per-batch average\n",
    "\n",
    "\t\t\topt.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\topt.step()\n",
    "\n",
    "\t\t\trunning += loss.item()\n",
    "\n",
    "\t\tif (epoch % 5 == 0 or epoch == epochs) and do_print:\n",
    "\t\t\tprint(f\"Epoch {epoch:03d} | loss: {running / len(loader):.4f}\")\n",
    "\n",
    "\tfeature_variance = np.var(x_prior, axis=0)\n",
    "\tfeature_mins = np.min(x_prior, axis=0)\n",
    "\tfeature_maxs = np.max(x_prior, axis=0)\n",
    "\tfeature_grids = []\n",
    "\tfor idx, var in enumerate(feature_variance):\n",
    "\t\tlo  = feature_mins[idx] - var      # lower bound  (min – variance)\n",
    "\t\thi  = feature_maxs[idx] + var      # upper bound  (max + variance)\n",
    "\t\tstep = var * 2 or 1e-8             # avoid step == 0 if var == 0\n",
    "\t\tgrid = np.arange(lo, hi + step, step)\n",
    "\t\tfeature_grids.append(grid)\n",
    "\n",
    "\t# --- Cartesian product  -------------------------------------------------\n",
    "\t# itertools.product is lazy ⇒ less memory than meshgrid on huge spaces\n",
    "\tsynthetic_X = np.fromiter(\n",
    "\t\t(val for combo in product(*feature_grids) for val in combo),\n",
    "\t\tdtype=float\n",
    "\t).reshape(-1, len(feature_grids))\n",
    "\n",
    "\tif do_print:\n",
    "\t\tprint(f\"{synthetic_X.shape[0]:,} synthetic rows × {synthetic_X.shape[1]} features\")\n",
    "\n",
    "\ttest_labels = torch.tensor([minority_label] * synthetic_X.shape[0])\n",
    "\n",
    "\tcvae.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\txb = torch.tensor(synthetic_X).float().to(device)\n",
    "\t\tmu, logvar = cvae.encode(xb)\n",
    "\t\tz = cvae.reparameterize(mu, logvar)\n",
    "\t\tsynthetic_X_DECODE = cvae.decode(z, test_labels)  \n",
    "\n",
    "\tsynthetic_Y = torch.tensor([minority_label] * synthetic_X.shape[0])\n",
    "\n",
    "\tproblem = NSGA_II_Filter(\n",
    "\t\tnp.concatenate((x_prior, synthetic_X_DECODE), axis=0), \n",
    "\t\tnp.concatenate((y_prior, synthetic_Y), axis=0), \n",
    "\t\tx_validation, y_validation,\n",
    "\t)\n",
    "\talgorithm = NSGA2(\n",
    "\t\tpop_size=500, \n",
    "\t\tsampling=DiverseSampling(), \n",
    "\t\tcrossover=HUX(), \n",
    "\t\tmutation=BitflipMutation(), \n",
    "\t\teliminate_duplicates=True\n",
    "\t)\n",
    "\tresult = minimize(\n",
    "\t\tproblem, \n",
    "\t\talgorithm, \n",
    "\t\t('n_gen', 10),\n",
    "\t\tsave_history=False,\n",
    "\t)\n",
    "\n",
    "\tcandidate_x = np.concatenate((x_prior, synthetic_X_DECODE), axis=0)\n",
    "\tcandidate_y = np.concatenate((y_prior, synthetic_Y), axis=0)\n",
    "\n",
    "\tmax_validation_auc = -1\n",
    "\tbest_x = None\n",
    "\tbest_y = None\n",
    "\n",
    "\tfor x in result.X:\n",
    "\t\tfiltered_x = candidate_x[x]\n",
    "\t\tfiltered_y = candidate_y[x]\n",
    "\t\t\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=5)\n",
    "\t\tmodel.fit(filtered_x, filtered_y)\n",
    "\t\ty_pred = model.predict(x_validation)\n",
    "\t\tauc = roc_auc_score(y_validation, y_pred)\n",
    "\n",
    "\t\tif auc > max_validation_auc:\n",
    "\t\t\tbest_x = filtered_x\n",
    "\t\t\tbest_y = filtered_y\n",
    "\t\t\tmax_validation_auc = auc\n",
    "\n",
    "\treturn best_x, best_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as fh:\n",
    "\tdata_mapper = pickle.load(fh)\n",
    "splits = pd.read_csv('data_splits.csv')\n",
    "\n",
    "data_keys = []\n",
    "for split_name in splits:\n",
    "\tfor idx in range(31):\n",
    "\t\tdata_keys.append(f\"{idx}_{split_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69677c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for dataset in splits:\n",
    "\tfor idx in range(31):\n",
    "\t\tdata_key = f\"{idx}_{dataset}\"\n",
    "\t\t\n",
    "\t\tif os.path.exists(f'results_new/{data_key}.pkl') is False: continue\n",
    "\n",
    "\t\twith open(f'results_new/{data_key}.pkl', 'rb') as fh:\n",
    "\t\t\tresults[data_key] = pickle.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e02e46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = []\n",
    "# dataset_name = \"abalone-17_vs_7-8-9-10\"\n",
    "dataset_name = \"abalone-20_vs_8-9-10\"\n",
    "dataset_name = \"abalone-21_vs_8\"\n",
    "for idx in range(31):\n",
    "\tdata_key = f\"{idx}_{dataset_name}\"\n",
    "\t\n",
    "\tif data_key not in results: continue\n",
    "\t\n",
    "\tx_test = data_mapper[data_key]['x_test']\n",
    "\ty_test = data_mapper[data_key]['y_test']\n",
    "\tx, y = results[data_key]\n",
    "\n",
    "\tmodel = KNeighborsClassifier(n_neighbors=5)\n",
    "\tmodel.fit(x, y)\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tcvae.append(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75183d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baselines = []\n",
    "smoted = []\n",
    "for idx in range(31):\n",
    "\tdata_key = f\"{idx}_{dataset_name}\"\n",
    "\tx_train = data_mapper[data_key]['x_train']\n",
    "\ty_train = data_mapper[data_key]['y_train']\n",
    "\n",
    "\tx_validation = data_mapper[data_key]['x_validation']\n",
    "\ty_validation = data_mapper[data_key]['y_validation']\n",
    "\n",
    "\tx_test = data_mapper[data_key]['x_test']\n",
    "\ty_test = data_mapper[data_key]['y_test']\n",
    "\n",
    "\tx_prior = np.concatenate((x_train, x_validation), axis=0)\n",
    "\ty_prior = np.concatenate((y_train, y_validation), axis=0)  \n",
    "\n",
    "\tmodel = KNeighborsClassifier(n_neighbors=5)\n",
    "\tmodel.fit(x_prior, y_prior)\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tbaselines.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "\ttry:\n",
    "\t\tx_smote, y_smote = SMOTE().fit_resample(x_prior, y_prior)\n",
    "\n",
    "\n",
    "\t\tmodel = KNeighborsClassifier(n_neighbors=5)\n",
    "\t\tmodel.fit(x_smote, y_smote)\n",
    "\t\ty_pred = model.predict(x_test)\n",
    "\t\tsmoted.append(roc_auc_score(y_test, y_pred))\n",
    "\texcept:\n",
    "\t\tsmoted.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf02c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t> B vs S pval 0.0074745082907966\n",
      "\t> B vs C pval 0.008297147842779923\n",
      "\t> S vs C pval 0.9775368152989685\n",
      "\t> S: 0.8466606088141754\n",
      "\t> B: 0.6988111464485841\n",
      "\t> C: 0.8424579736483416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\t> B vs S pval {ranksums(baselines, smoted).pvalue}\")\n",
    "print(f\"\\t> B vs C pval {ranksums(baselines, cvae).pvalue}\")\n",
    "print(f\"\\t> S vs C pval {ranksums(smoted, cvae).pvalue}\")\n",
    "print(f\"\\t> S: {np.mean(smoted)}\")\n",
    "print(f\"\\t> B: {np.mean(baselines)}\")\n",
    "print(f\"\\t> C: {np.mean(cvae)}\")\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
